{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Text Classification- Building a Tone Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bheeni Garg\n",
    "### August 31, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.102:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the spark context is working\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"text_tone.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------------+--------------------+\n",
      "|  tweet_id|  sentiment|         author|             content|\n",
      "+----------+-----------+---------------+--------------------+\n",
      "|1956967341|frustration|     xoshayzers|@tiffanylue i kno...|\n",
      "|1956967666|      anger|      wannamama|Layin n bed with ...|\n",
      "|1956967696|      anger|      coolfunky|Funeral ceremony....|\n",
      "|1956967789| excitement|    czareaquino|wants to hang out...|\n",
      "|1956968416|    neutral|      xkilljoyx|@dannycastillo We...|\n",
      "|1956968477|frustration|  xxxPEACHESxxx|Re-pinging @ghost...|\n",
      "|1956968487|      anger|       ShansBee|I should be sleep...|\n",
      "|1956968636|frustration|       mcsleazy|Hmmm. http://www....|\n",
      "|1956969035|      anger|    nic0lepaula|@charviray Charle...|\n",
      "|1956969172|      anger|     Ingenue_Em|@kelcouch I'm sor...|\n",
      "|1956969456|    neutral|     feinyheiny|    cant fall asleep|\n",
      "|1956969531|frustration|   dudeitsmanda|Choked on her ret...|\n",
      "|1956970047|      anger|       Danied32|Ugh! I have to be...|\n",
      "|1956970424|      anger|        Samm_xo|@BrodyJenner if u...|\n",
      "|1956970860| excitement|   okiepeanut93|        Got the news|\n",
      "|1956971077|      anger|         Sim_34|The storm is here...|\n",
      "|1956971170|  happiness|   poppygallico|@annarosekerr agreed|\n",
      "|1956971206|      anger|brokenangel1982|So sleepy again a...|\n",
      "|1956971473|frustration|          LCJ82|@PerezHilton lady...|\n",
      "|1956971586|      anger|        cleepow|How are YOU convi...|\n",
      "+----------+-----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tweet_id: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new dataset with only the necessary columns- `sentiment` and `content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfNew = df.select('sentiment', 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sentiment  |content                                                                                                                                   |\n",
      "+-----------+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|frustration|@tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[                                              |\n",
      "|anger      |Layin n bed with a headache  ughhhh...waitin on your call...                                                                              |\n",
      "|anger      |Funeral ceremony...gloomy friday...                                                                                                       |\n",
      "|excitement |wants to hang out with friends SOON!                                                                                                      |\n",
      "|neutral    |@dannycastillo We want to trade with someone who has Houston tickets, but no one will.                                                    |\n",
      "|frustration|Re-pinging @ghostridah14: why didn't you go to prom? BC my bf didn't like my friends                                                      |\n",
      "|anger      |I should be sleep, but im not! thinking about an old friend who I want. but he's married now. damn, &amp; he wants me 2! scandalous!      |\n",
      "|frustration|Hmmm. http://www.djhero.com/ is down                                                                                                      |\n",
      "|anger      |@charviray Charlene my love. I miss you                                                                                                   |\n",
      "|anger      |@kelcouch I'm sorry  at least it's Friday?                                                                                                |\n",
      "|neutral    |cant fall asleep                                                                                                                          |\n",
      "|frustration|Choked on her retainers                                                                                                                   |\n",
      "|anger      |Ugh! I have to beat this stupid song to get to the next  rude!                                                                            |\n",
      "|anger      |@BrodyJenner if u watch the hills in london u will realise what tourture it is because were weeks and weeks late  i just watch itonlinelol|\n",
      "|excitement |Got the news                                                                                                                              |\n",
      "|anger      |The storm is here and the electricity is gone                                                                                             |\n",
      "|happiness  |@annarosekerr agreed                                                                                                                      |\n",
      "|anger      |So sleepy again and it's not even that late. I fail once again.                                                                           |\n",
      "|frustration|@PerezHilton lady gaga tweeted about not being impressed by her video leaking just so you know                                            |\n",
      "|anger      |How are YOU convinced that I have always wanted you? What signals did I give off...damn I think I just lost another friend                |\n",
      "+-----------+------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNew.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNew.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 40000 tweets/documents that we use to build our classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformer - Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of taking text (such as a sentence) and breaking it into individual terms (usually words). RegexTokenizer allows more advanced tokenization based on regular expression (regex) matching. By default, the parameter “pattern” (regex, default: \"\\\\s+\") is used as delimiters to split the input text. Alternatively, users can set parameter “gaps” to false indicating the regex “pattern” denotes “tokens” rather than splitting gaps, and find all matching occurrences as the tokenization result.\n",
    "\n",
    "In Spark, Tokenizer class provides this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using RegexTokenizer here \n",
    "regexTokenizer = RegexTokenizer(inputCol=\"content\", outputCol=\"words\", pattern=\"\\\\W+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count the number of tokens/words in a tweet\n",
    "countTokens = udf(lambda words: len(words), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regexTokenized = regexTokenizer.transform(dfNew)\n",
    "dfTokenized = regexTokenized.withColumn(\"tokens\", countTokens(col(\"words\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+------+\n",
      "|  sentiment|             content|               words|tokens|\n",
      "+-----------+--------------------+--------------------+------+\n",
      "|frustration|@tiffanylue i kno...|[tiffanylue, i, k...|    17|\n",
      "|      anger|Layin n bed with ...|[layin, n, bed, w...|    11|\n",
      "|      anger|Funeral ceremony....|[funeral, ceremon...|     4|\n",
      "| excitement|wants to hang out...|[wants, to, hang,...|     7|\n",
      "|    neutral|@dannycastillo We...|[dannycastillo, w...|    15|\n",
      "+-----------+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTokenized.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWordsRemover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are words which should be excluded from the input, typically because the words appear frequently and don’t carry as much meaning.\n",
    "\n",
    "These are some of the most common, short function words, such as the, is, at, which, and on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+---------+\n",
      "|  sentiment|             content|               words|            filtered|filtokens|\n",
      "+-----------+--------------------+--------------------+--------------------+---------+\n",
      "|frustration|@tiffanylue i kno...|[tiffanylue, i, k...|[tiffanylue, know...|        9|\n",
      "|      anger|Layin n bed with ...|[layin, n, bed, w...|[layin, n, bed, h...|        7|\n",
      "|      anger|Funeral ceremony....|[funeral, ceremon...|[funeral, ceremon...|        4|\n",
      "| excitement|wants to hang out...|[wants, to, hang,...|[wants, hang, fri...|        4|\n",
      "|    neutral|@dannycastillo We...|[dannycastillo, w...|[dannycastillo, w...|        7|\n",
      "|frustration|Re-pinging @ghost...|[re, pinging, gho...|[re, pinging, gho...|       11|\n",
      "|      anger|I should be sleep...|[i, should, be, s...|[sleep, im, think...|       12|\n",
      "|frustration|Hmmm. http://www....|[hmmm, http, www,...|[hmmm, http, www,...|        5|\n",
      "|      anger|@charviray Charle...|[charviray, charl...|[charviray, charl...|        4|\n",
      "|      anger|@kelcouch I'm sor...|[kelcouch, i, m, ...|[kelcouch, m, sor...|        5|\n",
      "|    neutral|    cant fall asleep|[cant, fall, asleep]|[cant, fall, asleep]|        3|\n",
      "|frustration|Choked on her ret...|[choked, on, her,...| [choked, retainers]|        2|\n",
      "|      anger|Ugh! I have to be...|[ugh, i, have, to...|[ugh, beat, stupi...|        7|\n",
      "|      anger|@BrodyJenner if u...|[brodyjenner, if,...|[brodyjenner, u, ...|       13|\n",
      "| excitement|        Got the news|    [got, the, news]|         [got, news]|        2|\n",
      "|      anger|The storm is here...|[the, storm, is, ...|[storm, electrici...|        3|\n",
      "|  happiness|@annarosekerr agreed|[annarosekerr, ag...|[annarosekerr, ag...|        2|\n",
      "|      anger|So sleepy again a...|[so, sleepy, agai...|[sleepy, even, la...|        4|\n",
      "|frustration|@PerezHilton lady...|[perezhilton, lad...|[perezhilton, lad...|        8|\n",
      "|      anger|How are YOU convi...|[how, are, you, c...|[convinced, alway...|       10|\n",
      "+-----------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "dfSWRemoved = remover.transform(dfTokenized)\n",
    "dfSWR = dfSWRemoved.select('sentiment','content','words','filtered')\\\n",
    "     .withColumn(\"filtokens\", countTokens(col(\"filtered\")))\n",
    "dfSWR.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|filtered                                                                                            |\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[tiffanylue, know, listenin, bad, habit, earlier, started, freakin, part]                           |\n",
      "|[layin, n, bed, headache, ughhhh, waitin, call]                                                     |\n",
      "|[funeral, ceremony, gloomy, friday]                                                                 |\n",
      "|[wants, hang, friends, soon]                                                                        |\n",
      "|[dannycastillo, want, trade, someone, houston, tickets, one]                                        |\n",
      "|[re, pinging, ghostridah14, didn, go, prom, bc, bf, didn, like, friends]                            |\n",
      "|[sleep, im, thinking, old, friend, want, married, damn, amp, wants, 2, scandalous]                  |\n",
      "|[hmmm, http, www, djhero, com]                                                                      |\n",
      "|[charviray, charlene, love, miss]                                                                   |\n",
      "|[kelcouch, m, sorry, least, friday]                                                                 |\n",
      "|[cant, fall, asleep]                                                                                |\n",
      "|[choked, retainers]                                                                                 |\n",
      "|[ugh, beat, stupid, song, get, next, rude]                                                          |\n",
      "|[brodyjenner, u, watch, hills, london, u, realise, tourture, weeks, weeks, late, watch, itonlinelol]|\n",
      "|[got, news]                                                                                         |\n",
      "|[storm, electricity, gone]                                                                          |\n",
      "|[annarosekerr, agreed]                                                                              |\n",
      "|[sleepy, even, late, fail]                                                                          |\n",
      "|[perezhilton, lady, gaga, tweeted, impressed, video, leaking, know]                                 |\n",
      "|[convinced, always, wanted, signals, give, damn, think, lost, another, friend]                      |\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSWR.select('filtered').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Bag-of-Words using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-words comparisons are not very good when all tokens are treated the same: some tokens are more important than others.\n",
    "Weights give us a way to specify which tokens to favor. With weights, when we compare documents, instead of counting common tokens, we sum up the weights of common tokens. A good heuristic for assigning weights is called \"Term-Frequency/Inverse-Document-Frequency,\" or TF-IDF for short.\n",
    "\n",
    "#### TF\n",
    "TF rewards tokens that appear many times in the same document. It is computed as the frequency of a token in a document, that is, if document d contains 100 tokens and token t appears in d 5 times, then the TF weight of t in d is 5/100 = 1/20. The intuition for TF is that if a word occurs often in a document, then it is more important to the meaning of the document.\n",
    "\n",
    "#### IDF\n",
    "IDF rewards tokens that are rare overall in a dataset. The intuition is that it is more significant if two documents share a rare word than a common one. IDF weight for a token, t, in a set of documents, U, is computed as follows:\n",
    "\n",
    "#### Let N be the total number of documents in U\n",
    "#### Find n(t), the number of documents in U that contain t\n",
    "#### Then IDF(t) = N/n(t). \n",
    "#### Note that n(t)/N is the frequency of t in U, and N/n(t) is the inverse frequency.\n",
    "\n",
    "Note on terminology: Sometimes token weights depend on the document the token belongs to, that is, the same token may have a different weight when it's found in different documents. We call these weights local weights. TF is an example of a local weight, because it depends on the length of the source. On the other hand, some token weights only depend on the token, and are the same everywhere that token is found. We call these weights global, and IDF is one such weight.\n",
    "\n",
    "#### TF-IDF\n",
    "Finally, to bring it all together, the total TF-IDF weight for a token in a document is the product of its TF and IDF weights.\n",
    "\n",
    "[TF-IDF in Spark](https://spark.apache.org/docs/2.1.0/ml-features.html#tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sentiment  |filtokens|rawFeatures                                                                                                                      |\n",
      "+-----------+---------+---------------------------------------------------------------------------------------------------------------------------------+\n",
      "|frustration|9        |(50000,[7086,7522,9561,21217,23207,39586,45192,47779,48740],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                               |\n",
      "|anger      |7        |(50000,[2199,11359,21655,35146,47985,48354,48602],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                 |\n",
      "|anger      |4        |(50000,[36480,38834,39290,43328],[1.0,1.0,1.0,1.0])                                                                              |\n",
      "|excitement |4        |(50000,[4442,15928,31041,48062],[1.0,1.0,1.0,1.0])                                                                               |\n",
      "|neutral    |7        |(50000,[17712,22044,28355,31136,35531,40423,44750],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                |\n",
      "|frustration|11       |(50000,[2402,2425,4442,6946,18077,20141,21369,22606,23330,39147],[1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0])                      |\n",
      "|anger      |12       |(50000,[3688,3998,17712,17989,21961,27238,30775,32682,37002,37672,48062,48187],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|frustration|5        |(50000,[6757,13793,23018,26221,30665],[1.0,1.0,1.0,1.0,1.0])                                                                     |\n",
      "|anger      |4        |(50000,[6240,21071,45242,49765],[1.0,1.0,1.0,1.0])                                                                               |\n",
      "|anger      |5        |(50000,[7757,10520,43328,46638,48026],[1.0,1.0,1.0,1.0,1.0])                                                                     |\n",
      "|neutral    |3        |(50000,[6026,13729,46367],[1.0,1.0,1.0])                                                                                         |\n",
      "|frustration|2        |(50000,[3190,21802],[1.0,1.0])                                                                                                   |\n",
      "|anger      |7        |(50000,[6814,7350,8273,11959,22235,22569,49121],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                   |\n",
      "|anger      |13       |(50000,[2607,10277,13079,13455,18330,20688,27526,33202,37312,48701],[1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2.0])                   |\n",
      "|excitement |2        |(50000,[16007,42433],[1.0,1.0])                                                                                                  |\n",
      "|anger      |3        |(50000,[15536,35154,44732],[1.0,1.0,1.0])                                                                                        |\n",
      "|happiness  |2        |(50000,[15471,15541],[1.0,1.0])                                                                                                  |\n",
      "|anger      |4        |(50000,[13079,26885,36406,44532],[1.0,1.0,1.0,1.0])                                                                              |\n",
      "|frustration|8        |(50000,[8418,17670,17857,21526,23464,31804,47779,48351],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                       |\n",
      "|anger      |10       |(50000,[1564,3998,5091,7013,19395,23779,23783,27238,41233,44704],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                      |\n",
      "+-----------+---------+---------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# implementing TF\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=50000)\n",
    "tf = hashingTF.transform(dfSWR)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "tf.select('sentiment', 'filtokens', 'rawFeatures').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sentiment  |features                                                                                                                                                                                                                                                                                                  |\n",
      "+-----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|frustration|(50000,[7086,7522,9561,21217,23207,39586,45192,47779,48740],[4.099884742597716,7.5056172794252625,8.650749583728265,6.907780278669642,6.422272462887942,9.49804744411547,5.971686919499308,3.409002568668623,5.878160861488484])                                                                          |\n",
      "|anger      |(50000,[2199,11359,21655,35146,47985,48354,48602],[5.566221811391143,7.229363902797105,5.028315229022482,5.128599591648447,8.294074639789534,4.337078268718655,9.49804744411547])                                                                                                                         |\n",
      "|anger      |(50000,[36480,38834,39290,43328],[7.3777839079153775,7.95760240316832,8.294074639789534,4.860087435304386])                                                                                                                                                                                               |\n",
      "|excitement |(50000,[4442,15928,31041,48062],[4.735873509317713,6.17781912498698,4.539875719554954,5.592713426838119])                                                                                                                                                                                                 |\n",
      "|neutral    |(50000,[17712,22044,28355,31136,35531,40423,44750],[3.695929068738406,3.403725511567779,6.119322918305372,7.824071010543797,8.198764459985208,9.210365371663688,4.930233044671146])                                                                                                                       |\n",
      "|frustration|(50000,[2402,2425,4442,6946,18077,20141,21369,22606,23330,39147],[8.987221820349479,3.845558263846818,4.735873509317713,8.987221820349479,3.1361694269582414,8.720580285159748,7.162672528298432,7.041311671294165,3.064571589241856,6.453525006392046])                                                  |\n",
      "|anger      |(50000,[3688,3998,17712,17989,21961,27238,30775,32682,37002,37672,48062,48187],[7.300822866779249,4.896216159392893,3.695929068738406,3.804315305312769,4.237085864111201,4.850456542243426,5.323660174219832,3.483517624076491,3.6700826995608535,4.863318455885833,5.592713426838119,9.210365371663688])|\n",
      "|frustration|(50000,[6757,13793,23018,26221,30665],[9.49804744411547,5.26394093951821,6.57130804204843,3.40147241260487,3.0911674580461543])                                                                                                                                                                           |\n",
      "|anger      |(50000,[6240,21071,45242,49765],[3.2429374023839475,3.823579357128043,9.210365371663688,9.903512552223633])                                                                                                                                                                                               |\n",
      "|anger      |(50000,[7757,10520,43328,46638,48026],[5.268783563993997,4.058519909015908,4.860087435304386,2.5536388474852965,9.903512552223633])                                                                                                                                                                       |\n",
      "+-----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# implementing IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(tf)\n",
    "tfidf = idfModel.transform(tf)\n",
    "\n",
    "tfidf.select('sentiment', 'features').show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "|  sentiment|             content|               words|            filtered|filtokens|         rawFeatures|            features|\n",
      "+-----------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "|frustration|@tiffanylue i kno...|[tiffanylue, i, k...|[tiffanylue, know...|        9|(50000,[7086,7522...|(50000,[7086,7522...|\n",
      "|      anger|Layin n bed with ...|[layin, n, bed, w...|[layin, n, bed, h...|        7|(50000,[2199,1135...|(50000,[2199,1135...|\n",
      "|      anger|Funeral ceremony....|[funeral, ceremon...|[funeral, ceremon...|        4|(50000,[36480,388...|(50000,[36480,388...|\n",
      "| excitement|wants to hang out...|[wants, to, hang,...|[wants, hang, fri...|        4|(50000,[4442,1592...|(50000,[4442,1592...|\n",
      "|    neutral|@dannycastillo We...|[dannycastillo, w...|[dannycastillo, w...|        7|(50000,[17712,220...|(50000,[17712,220...|\n",
      "+-----------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF = tfidf.select('sentiment', 'content','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|  sentiment|             content|            features|\n",
      "+-----------+--------------------+--------------------+\n",
      "|frustration|@tiffanylue i kno...|(50000,[7086,7522...|\n",
      "|      anger|Layin n bed with ...|(50000,[2199,1135...|\n",
      "|      anger|Funeral ceremony....|(50000,[36480,388...|\n",
      "| excitement|wants to hang out...|(50000,[4442,1592...|\n",
      "|    neutral|@dannycastillo We...|(50000,[17712,220...|\n",
      "|frustration|Re-pinging @ghost...|(50000,[2402,2425...|\n",
      "|      anger|I should be sleep...|(50000,[3688,3998...|\n",
      "|frustration|Hmmm. http://www....|(50000,[6757,1379...|\n",
      "|      anger|@charviray Charle...|(50000,[6240,2107...|\n",
      "|      anger|@kelcouch I'm sor...|(50000,[7757,1052...|\n",
      "|    neutral|    cant fall asleep|(50000,[6026,1372...|\n",
      "|frustration|Choked on her ret...|(50000,[3190,2180...|\n",
      "|      anger|Ugh! I have to be...|(50000,[6814,7350...|\n",
      "|      anger|@BrodyJenner if u...|(50000,[2607,1027...|\n",
      "| excitement|        Got the news|(50000,[16007,424...|\n",
      "|      anger|The storm is here...|(50000,[15536,351...|\n",
      "|  happiness|@annarosekerr agreed|(50000,[15471,155...|\n",
      "|      anger|So sleepy again a...|(50000,[13079,268...|\n",
      "|frustration|@PerezHilton lady...|(50000,[8418,1767...|\n",
      "|      anger|How are YOU convi...|(50000,[1564,3998...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting, transforming and selecting features- StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now convert the sentiments which are of type string to categorical variables or 'classes' to create labels/ responses for building the classification model. \n",
    "\n",
    "This is achieved using the StringIndexer class in Spark. [String Indexer](https://spark.apache.org/docs/2.1.0/ml-features.html#stringindexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-----+\n",
      "|  sentiment|             content|            features|label|\n",
      "+-----------+--------------------+--------------------+-----+\n",
      "|frustration|@tiffanylue i kno...|(50000,[7086,7522...|  1.0|\n",
      "|      anger|Layin n bed with ...|(50000,[2199,1135...|  3.0|\n",
      "|      anger|Funeral ceremony....|(50000,[36480,388...|  3.0|\n",
      "| excitement|wants to hang out...|(50000,[4442,1592...|  4.0|\n",
      "|    neutral|@dannycastillo We...|(50000,[17712,220...|  2.0|\n",
      "|frustration|Re-pinging @ghost...|(50000,[2402,2425...|  1.0|\n",
      "|      anger|I should be sleep...|(50000,[3688,3998...|  3.0|\n",
      "|frustration|Hmmm. http://www....|(50000,[6757,1379...|  1.0|\n",
      "|      anger|@charviray Charle...|(50000,[6240,2107...|  3.0|\n",
      "|      anger|@kelcouch I'm sor...|(50000,[7757,1052...|  3.0|\n",
      "|    neutral|    cant fall asleep|(50000,[6026,1372...|  2.0|\n",
      "|frustration|Choked on her ret...|(50000,[3190,2180...|  1.0|\n",
      "|      anger|Ugh! I have to be...|(50000,[6814,7350...|  3.0|\n",
      "|      anger|@BrodyJenner if u...|(50000,[2607,1027...|  3.0|\n",
      "| excitement|        Got the news|(50000,[16007,424...|  4.0|\n",
      "|      anger|The storm is here...|(50000,[15536,351...|  3.0|\n",
      "|  happiness|@annarosekerr agreed|(50000,[15471,155...|  0.0|\n",
      "|      anger|So sleepy again a...|(50000,[13079,268...|  3.0|\n",
      "|frustration|@PerezHilton lady...|(50000,[8418,1767...|  1.0|\n",
      "|      anger|How are YOU convi...|(50000,[1564,3998...|  3.0|\n",
      "+-----------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n",
    "indexed = indexer.fit(dataDF).transform(dataDF)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentiment: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the label encoding as performed by the String Indexer class-\n",
    "\n",
    "* Happiness: 0.0 \n",
    "* Frustration: 1.0 \n",
    "* Neutral: 2.0 \n",
    "* Anger: 3.0 \n",
    "* Excitement: 4.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data 80/20 into training and test data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed the Feature Engineering phase, we move on to splitting our dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF, testDF = indexed.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Multinomial Naive Bayes classifier to the training dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes’ theorem with strong (naive) independence assumptions between the features. The spark.ml implementation currently supports both multinomial naive Bayes and Bernoulli naive Bayes.\n",
    "\n",
    "[Paper on Multinomial NB](http://www.cs.cmu.edu/~knigam/papers/multinomial-aaaiws98.pdf)\n",
    "\n",
    "[Naive Bayes in Spark](https://spark.apache.org/docs/latest/ml-classification-regression.html#naive-bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# fit the naive bayes model to the training set\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------+----------+\n",
      "|probability                                                                                                   |prediction|\n",
      "+--------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[7.751364105151501E-10,7.165477198045954E-5,5.154244350756452E-8,6.753444062239701E-11,0.9999282928429053]    |4.0       |\n",
      "|[1.6668974717695789E-12,0.0018377450004126738,0.0013443703748229178,0.9968178846230855,1.1992943613046301E-14]|3.0       |\n",
      "|[3.650374493438045E-18,0.9999888913420104,1.2347678596587943E-12,1.1108656754684378E-5,3.6286687293873124E-24]|1.0       |\n",
      "|[4.312817636314641E-28,0.9999999999918163,4.43928990613844E-12,6.457174429445445E-22,3.744276976374241E-12]   |1.0       |\n",
      "|[9.537300505493539E-14,0.009320853345868328,4.391185329033474E-13,0.990679146639608,1.3989400777009734E-11]   |3.0       |\n",
      "|[1.0132207380124788E-29,0.9859535188931098,0.014046481106890262,2.8979041674161443E-20,2.7407979869452534E-20]|1.0       |\n",
      "|[0.9990659780828279,9.33701724197954E-4,3.116763081788427E-7,4.743651042151604E-9,3.773014719934359E-9]       |0.0       |\n",
      "|[3.3668764338625725E-15,5.581762851021779E-9,2.2534549440482077E-10,8.227372667985E-4,0.9991772569260899]     |4.0       |\n",
      "|[0.02768829209678829,6.121409810410236E-6,0.06603310864740576,0.007200606028639197,0.8990718718173564]        |4.0       |\n",
      "|[9.378763730762727E-40,0.9999999711466773,2.8853285469063422E-8,5.674197888189511E-16,3.6581254008473106E-14] |1.0       |\n",
      "+--------------------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predicting probabilities and labels on the test set\n",
    "# select example rows to display.\n",
    "predictions = model.transform(testDF)\n",
    "predictions.select('probability', 'prediction').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.313319186119\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial NB model gives an accuracy of about 31% which is low. \n",
    "\n",
    "I have built a very crude model to lay a foundation of the analyzer. My next step will include refining the model in order to improve accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "An ML pipeline can now be created to model the training dataset with labels and test the built model on a test set without labels. In other words, we feed in text/documents and the analyzer throws out the predicted tone of the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'i am glad you came', '[0.999293805414,0.000290829745061,4.2468694878e-05,0.000205155000072,0.000167741145984]', '0.0'\n",
      "'sooo look forward to this match today!!', '[0.326912215958,0.657931619476,0.000213437832279,5.63099231622e-05,0.0148864168107]', '1.0'\n",
      "'i gave him an enthusiastic smile.', '[0.0873458054923,1.24295920149e-14,5.57930922629e-06,0.91264861519,8.86301451627e-12]', '3.0'\n",
      "'you were never to step out of that door', '[1.03265854052e-05,0.157013476744,0.497622282685,0.00351662579954,0.341837288186]', '2.0'\n",
      "'it was a horrific rain', '[5.00123142958e-14,0.979025836093,2.33895748233e-12,0.0209741638696,3.48286487033e-11]', '1.0'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, remover, hashingTF, idf, indexer, nb])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(dfNew)\n",
    "\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (\"i am glad you came\"),\n",
    "    (\"sooo look forward to this match today!!\"),\n",
    "    (\"i gave him an enthusiastic smile.\"),\n",
    "    (\"you were never to step out of that door\"),\n",
    "    (\"it was a horrific rain\")\n",
    "], StringType())\n",
    "\n",
    "testdf = test.selectExpr(\"value as content\")\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(testdf)\n",
    "selected = prediction.select(\"content\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    content, prob, prediction = row\n",
    "    #print(\"(%s, %s) --> prob=%s, prediction=%f\" % (content, str(prob), prediction))\n",
    "    print(\"'{0}', '{1}', '{2}'\".format(content, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier here is a weak classifier which gives an accuracy of about 31%. With this accuracy it is able to somewhat correctly predict the tone of the first and last sentences but really goes off tangent for the rest. There is a need to refine the model further and/or try different algorithms which might improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. [How to use spark Naive Bayes classifier for text classification with IDF?](https://stackoverflow.com/questions/32231049/how-to-use-spark-naive-bayes-classifier-for-text-classification-with-idf)\n",
    "\n",
    "2. [Sentiment Analysis PySpark](https://github.com/nisarg64/Sentiment-Analysis-Pyspark/blob/master/text_analysis.py)\n",
    "\n",
    "3. [mastering-apache-spark-book](https://github.com/jaceklaskowski/mastering-apache-spark-book/blob/master/spark-mllib/spark-mllib-pipelines-example-classification.adoc)\n",
    "\n",
    "4. [Apache Spark Documentation](https://spark.apache.org/docs/2.1.0/ml-features.html#tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
